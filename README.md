# 🏠 MLOps Pipeline for California Housing Price Prediction

This project demonstrates a full end-to-end MLOps workflow using the California Housing dataset with the following technologies:

- ✅ **Git + DVC** for data & model versioning
- 📊 **MLflow** for experiment tracking
- ⚙️ **FastAPI** for model serving
- 🐳 **Docker** for containerization
- 🔁 **GitHub Actions** for CI/CD
- 🔍 **Logging & Monitoring** with `/metrics` endpoint

---

## 📁 Folder Structure

```
mlops-housing/
├── data/                # Contains housing.csv (tracked via DVC)
├── models/              # Stores best_model.pkl (generated by training)
├── logs/                # API logs
├── src/                 # Python source code
│   ├── train.py         # Model training and MLflow logging
│   └── app.py           # FastAPI inference app
├── params.yaml          # Model hyperparameters
├── dvc.yaml             # DVC pipeline
├── Dockerfile           # Docker container definition
├── requirements.txt     # Python dependencies
├── .github/workflows/ci-cd.yml  # GitHub Actions CI/CD
```

---

## 🛠️ Prerequisites

- Python ≥ 3.10
- Git
- DVC (`pip install dvc`)
- Docker
- GitHub account + Docker Hub account

---

## 🚀 Step-by-Step Execution

### 1️⃣ Clone the Project or Unzip

```bash
unzip mlops-housing.zip
cd mlops-housing
```

### 2️⃣ Create a Virtual Environment and Install Dependencies

```bash
python -m venv venv
source venv/bin/activate        # Windows: venv\Scripts\activate
pip install -r requirements.txt
```

### 3️⃣ Generate and Save the Dataset (Run Once)

```python
# save_dataset.py
from sklearn.datasets import fetch_california_housing
import pandas as pd
df = fetch_california_housing(as_frame=True).frame
df.rename(columns={'MedHouseVal': 'Price'}, inplace=True)
df.to_csv("data/housing.csv", index=False)
```

Then run:

```bash
python save_dataset.py
```

### 4️⃣ Initialize DVC and Track the Dataset

```bash
dvc init
dvc add data/housing.csv
echo "data/housing.csv" >> .gitignore
git add data/housing.csv.dvc .gitignore
git commit -m "Track dataset with DVC"
```

### 5️⃣ Reproduce the Pipeline with DVC

```bash
dvc repro
```

✅ This runs training and saves `models/best_model.pkl`.

---

## 📊 View Experiments in MLflow

```bash
mlflow ui
```

Then open [http://localhost:5000](http://localhost:5000) in your browser.

---

## 🌐 Serve the Model with FastAPI

```bash
uvicorn src.app:app --reload
```

Visit: [http://localhost:8000/docs](http://localhost:8000/docs) to test `/predict`.

Example payload:

```json
{
  "MedInc": 8.3,
  "HouseAge": 20,
  "AveRooms": 6,
  "AveBedrms": 1,
  "Population": 500,
  "AveOccup": 2.5,
  "Latitude": 37,
  "Longitude": -122
}
```

✅ You’ll get a prediction like:

```json
{"prediction": 2.85}
```

---

## 📈 Monitor Metrics

Visit: [http://localhost:8000/metrics](http://localhost:8000/metrics)

Output:
```
predictions_total 5
```

---

## 🐳 Build and Run Docker Image

```bash
docker build -t housing-api .
docker run -p 8000:8000 housing-api
```

---

## 🔁 Set Up GitHub Actions CI/CD

1. Push project to a GitHub repository:

```bash
git remote add origin https://github.com/YOUR_USERNAME/mlops-housing.git
git branch -M main
git push -u origin main
```

2. Add GitHub Secrets:
   - `DOCKER_USERNAME`
   - `DOCKER_PASSWORD`

3. On push, GitHub Actions will:
   - Run training
   - Build Docker image
   - Push to Docker Hub

---

## 🔁 Re-run with Different Hyperparameters

Edit `params.yaml`:

```yaml
model:
  max_depth: 10
```

Then rerun:

```bash
dvc repro
```

---

## ✅ Summary

| Task                     | Tool Used        |
|--------------------------|------------------|
| Data versioning          | DVC              |
| Code versioning          | Git              |
| Training                 | Scikit-learn     |
| Experiment tracking      | MLflow           |
| Serving                  | FastAPI          |
| Deployment               | Docker           |
| Automation               | GitHub Actions   |
| Monitoring               | Logging + Metrics|

---

## 📬 Questions?

Feel free to reach out or raise an issue if you'd like help deploying this to EC2, Render.com, or adding Prometheus/Grafana dashboards.
